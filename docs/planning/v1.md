# Introduction & Outlining Context:

As generative AI is becoming increasingly popular and the open source models that the average user is *technically* able to run at home become more powerful, the seemingly natural conclusion is that the hardware required to run these models will increase massively

As it stands, the most common way standard casual users interact with generative AI is via a web UI that mimics the look and feel of an instant messaging chat with the model where you provide input and it provides output

As it stands that is an awesome way to quickly get the answer to your question in a way that feels like asking a question to a person over a message. 

However, when trying to replicate the real time chat experience that services like OpenAI, Google or Anthropic, where the question is answered fairly quickly and in what feels like a reasonable amount of time for the experience, using something like Ollama + Open UI on consumer hardware, it is naturally going to lag

This in my opinion simply makes the UI/UX experience of working with self hosted models on my broke boy hardware, less desirable and frankly not worth it.

TL;DR: I don’t like that

I like self hosting & owning my data, so that is the overall goal in what I am about to propose 

## Idea intro

Basically the idea is to shift the expectations of the user interacting with these models.

There is nothing I can do to make the models lighter and I can’t just keep buying more hardware

So the goal is to change what I can, being the user experience & expectations by modifying how they interact with the models, and ultimately the timeframe in which they expect a response 

To do this, my proposal is to switch from a UX reminiscent of instant messaging to a UX comparable to that of the modern email experience 

For this I intend to make use of existing reputable open source self hosted software

The idea is you would interact with these models through something like thunderbird 

The models themselves will be discussed in more detail later but they work using python & SMTP client libraries like salmon maybe 

We will use postfix for the server because I like postfix 

Anyway back to UI/UX

The idea is that, typically when you send an email, you put more thought & information into the email than a text, and expect the same in return 

This simply takes more time and in addition to the “work” feeling of email, your expectations of when you will get an email back from the model is simply different than watching a stream of text slowly load on your screen over 5 minutes 

Now that I have begun to establish a new dynamic between these generative AI models, and users of this system, I believe it is important that we heavily lean into this dynamic and make consistent efforts to maximize the benefits of this dynamic and soften the downsides in that we want to be transparent about the potential downsides like slow response times but make the response worth the wait in the end. 

Generally speaking I want the prompts to be well thought out and more detailed than one sentence but in return the user should feel the information they receive from the model was worth the time and wait for it to be generated 

to make sure the responses are as optimized as possible for what the user is looking for, we are going to make response generation a multi model process, which attempts to make use of the wide range of open source AI/ML models as well as common AI tools like search engine abilities, API access to relevant services, database access, CSV & office document management tools, and more, allowing for a wide variety of data to be referenced 

We are looking to make this system more of a RAG than standard LLM chatbot

In fact rather than interacting with models as ML models, like how you might address ChatGPT as chatGPT, the models should aim to take on specific personas similar to open AIs GPT builder which we can likely emulate using ollama model files


# Technical Details

## UI/UX
Thunderbird email client

## Server Software

### Postfix 
The `SMTP` server that will be used to send and receive emails from the models. This will be the primary way that the user interacts with the models, by sending emails to the models and receiving responses from the models.

### Ollama
The `Ollama` models that will be used to generate responses to user prompts. These models will be trained on a wide variety of data and will be able to access a wide variety of data sources to generate responses.

### This platform, running as a systemd service

## Models
Models are the core of the system, they are the AI/ML models that will be used to generate responses to user prompts. These models will be trained on a wide variety of data and will be able to access a wide variety of data sources to generate responses.
We will get our models from [Ollama](), [Huggingface](), and other reputable open source model providers to build our applciation

## Data Sources
Data sources are the sources of data that the models will be able to access to generate responses. These data sources will be a wide variety of sources, including but not limited to, search engines, APIs, databases, CSV & office documents, and more.

## Agents

`Agents`, for the sake of this project, are the personas that the models will take on when interacting with the user. They will be a collection of `prompts`, `models`, and other related tools, that users can combine to create an agent. 

this agent can be used to generate responses to user prompts, and can be used to generate responses to user prompts.

The agents will be given accounts on the postfix server, and will be able to send and receive emails from the user as if they were themselves a user.

## Response Generation Techniques

I want to approach the response generation in a slow, iterative process in which the system preforms extremely detailed analysis, builds a plan for how to reply, then replies in the most effective way possible, while citing sources, creating files/attachments and more.


```python

class Response:
    def __init__(self, response_subject_line: str, response_body: str, response_attachments: List[str], response_body_sources: List[str]):
        self.response_subject_line = response_subject_line
        self.response_body = response_body
        self.response_attachments = response_attachments
        self.response_body_sources = response_body_sources

async def main() -> None:

    
    ## read the latest messages
    messages = await read_messages()

    ## analyze the messages
    analysis = await analyze_messages(messages)

    ## build a plan for how to reply

    plan = await build_plan(analysis)


    ## create the response draft variable
    response_draft: str = ""

    ## iterate over the plan and build the response draft
    for step in plan:
        response_draft += step

    ## preform a quality check on the response draft
    response_draft_quality_score = await quality_check(response_draft)
    ## if it is good, send it to the user 
    if response_draft_quality_score > 0.8:
        
        ## send the response draft to the user
        await send_response(response_draft)
    elif response_draft_quality_score < 0.8:
        ## if it is not good, have the model explain why and what to improve.
        await explain_and_improve(response_draft)

    else:
        ## if it is not good, have the model explain why and what to improve.
        await explain_and_improve(response_draft)

```



This is of course a rough draft of pseduo-code, and does not represent the goals of this system perfectly, but it is a good starting point for how we can approach the response generation process.



## Additional Inspiration


### [Storm](https://github.com/stanford-oval/storm)
One tool I want to emulate heavily within the overall abilities of this system, is that of Storm, the LLM based research tool from Standford Uni 

https://github.com/stanford-oval/storm

This system is designed to create reports similar to Wikipedia, with citations and everything 

I am wanting to replicate similar professionalism and comprehensive reporting. I want my system to be able to tell a good source from a bad source and provide links to where it got information 

Including quotes is cool, but not required 

Something worth noting is that I am also interested in enhancing the ability to create reliable outlines and instructions for complex software development solutions 

I would like to be able to feed my system either a link to, or a downloaded copy of the latest/most relevant information and documentation for the task. I mean I want to be able to email the agent a zip containing all the docs for a certain version of a programming language or library or whatever


## Quality Control


I want to give my system some form of internal testing tool to ensure that it stays on topic & does not go off on unrelated tangents



----
---
---



# Refining Into a Business Plan


I want to create a business that seeks to monetize a variety of LLMs 

The main idea of the business is that it shifts the users expectations for when they will receive a response to their prompt by using email as the primary method of interacting with the models 

This way, rather than waiting as text slowly generates across a screen in real time, users will instead receive an email back when the message is completely generated 


users can provide the model with attachments, links and other hypermedia in their email messages

The models will ideally be able to provide more detailed information and overall longer replies especially when using self hosted models via ollama 


In terms of features I want to make it so the models can browse the web and have memories as well as reference internal knowledge bases for business and organizations allowing them to more effectively onboard new employees 

Our target audience is businesses with large internal knowledge bases and individuals who prefer email over the standard LLM chat bot interface



## Business Model

The business model is a subscription based model where users pay a monthly fee to access the service.

The service will be available in three tiers, with the lowest tier being free and the highest tier being the most expensive.

The free tier will have limited access to the models and will have a limit on the number of emails that can be sent per month.

The middle tier will have access to more models and will have a higher limit on the number of emails that can be sent per month.

The highest tier will have access to all models and will have no limit on the number of emails that can be sent per month.

The business tier will also include a pay per use option, where users can pay a fee for each email that is sent.


## Marketing Strategy

The marketing strategy will be focused on targeting businesses with large internal knowledge bases and individuals who prefer email over the standard LLM chat bot interface.

The marketing strategy will include targeted advertising on social media platforms, email marketing campaigns, and partnerships with other businesses.

The marketing strategy will also include a referral program, where users can refer other users to the service and receive a discount on their subscription.



# Revised Tech Stack

## UI/UX
- BYO Email Client - Clients can email from wherever they want, but we will provide a guide on how to set up Thunderbird for the best experience, and we will provide a web UI for account management and interacting with the service itself
- Web UI for account management, and interacting with the service itself, which will be built using
  - React
  - Vite
  - Tailwind CSS
  - TypeScript
  - [Litestar Python ASGI Library](https://litestar.dev) for the Backend
  - [Shadcn UI Components](https://ui.shadcn.com) for the web UI
- [Postfix](https://www.postfix.org/) for the SMTP server
- [Ollama](https://ollama.com) for the models
- [Huggingface](https://huggingface.co) for additional models
- [Brave Search API](https://search.brave.com) for search engine access (potentially more if needed)


















